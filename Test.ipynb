{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import selenium.common.exceptions as ex\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "\n",
    "\n",
    "def create_initial_empty_df(df_type):\n",
    "    '''Create empty dataframes to append new jobs and keywords data to.'''\n",
    "\n",
    "    if df_type == 'job':\n",
    "        return pd.DataFrame(columns=['id', 'job_title', 'company', 'location', 'rating_provided', 'rating', 'salary_provided', 'salary_text', 'weblink', 'date_recorded']).set_index('id')\n",
    "    if df_type == 'keyword':\n",
    "        return pd.DataFrame(columns=['id', 'keyword']).set_index('id')\n",
    "\n",
    "def get_searchable_dict(dict_type, search_term = None):\n",
    "    '''Create a dictionary with the existing values from SQL as the keys. Used to do hash lookups to ensure only unique values are inserted back into the database.'''\n",
    "    engine = sqlalchemy.create_engine('sqlite:///JobData.db')\n",
    "    if dict_type == 'job':\n",
    "        return pd.read_sql('SELECT id, row_number() OVER () AS row_num FROM Jobs', engine, index_col= 'id')['row_num'].to_dict()\n",
    "    if dict_type == 'keyword':\n",
    "        return pd.read_sql(f\"SELECT id, row_number() OVER () AS row_num FROM KeywordRef WHERE keyword = '{search_term}'\", engine, index_col= 'id')['row_num'].to_dict()\n",
    "\n",
    "\n",
    "def create_firefox_driver(headless = False):\n",
    "    '''Open Firefox webpage usable by program to navigate to webpages'''\n",
    "    options = Options()\n",
    "    options.headless = headless\n",
    "    driver = webdriver.Firefox(options = options)\n",
    "    return driver\n",
    "\n",
    "def click_to_next_page(driver):\n",
    "    '''Click the next page button, moving to the next page of jobs.'''\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@data-testid=\"pagination-page-next\"]'))).click()\n",
    "        return True\n",
    "    except (ex.TimeoutException, ex.ElementClickInterceptedException):\n",
    "        return False\n",
    "\n",
    "def close_email_popup(driver):\n",
    "    '''Clears email popup if prompted on indeed during web scrape.'''\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//*[@class=\"DesktopJobAlertPopup-heading\"]'))).click()\n",
    "        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//*[@class=\"css-yi9ndv e8ju0x51\"]'))).click()\n",
    "        return True\n",
    "    except ex.TimeoutException:\n",
    "        return False\n",
    "\n",
    "def bypass_cloudflare_check(driver):\n",
    "    '''In rare instances, a cloudflare check comes up on Indeed. This will bypass the cloudflare check if it comes up.'''\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.frame_to_be_available_and_switch_to_it((By.XPATH,\"//iframe[@title='Widget containing a Cloudflare security challenge']\")))\n",
    "        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//label[@class='ctp-checkbox-label']\"))).click()\n",
    "        time.sleep(10)\n",
    "        return True\n",
    "    except ex.TimeoutException:\n",
    "        return False\n",
    "\n",
    "def parse_webpage_html(driver):\n",
    "    '''\n",
    "    Parse the page to extract the html code blocks related to the listings on the left side of the webpage. \n",
    "    Creates a list with each element in the list containing the information for each listing.\n",
    "    '''\n",
    "    source = driver.page_source\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    job_list = soup.find_all('li', class_ = 'css-5lfssm eu4oa1w0')\n",
    "    return job_list\n",
    "\n",
    "\n",
    "def job_is_ad(job):\n",
    "    '''Ensure element extracted is a job rather than an ad by looking for a class unique to jobs only.'''\n",
    "    ad_container = job.find('div', class_ = 'mosaic-zone nonJobContent-desktop')\n",
    "    return ad_container\n",
    "\n",
    "def get_job_uid(job, id_dict):\n",
    "    '''Parse out job uid from html string. If job id is not found or it already exists in database then None is retured to tell the program to ignore that job element.'''\n",
    "    job_uid = job.find('span')['id'].split('-')[1]\n",
    "    if job_uid is None:\n",
    "        return None\n",
    "    if job_uid in id_dict:\n",
    "        return None\n",
    "    return job_uid\n",
    "\n",
    "def get_job_title(job):\n",
    "    '''Parse out job title from html string'''\n",
    "    return job.find('h2').text\n",
    "\n",
    "def get_job_company(job):\n",
    "    '''Parse out job company from html string'''\n",
    "    return job.find('span', {'data-testid': 'company-name'}).text\n",
    "\n",
    "def get_job_location(job):\n",
    "    '''Parse out job location from html string'''\n",
    "    return job.find('div', {'data-testid' : 'text-location'}).text\n",
    "\n",
    "def get_company_rating(job):\n",
    "    '''Parse out company rating of job from html string. Most companies have no rating, so a y/n field is also passed back to differentiate.'''\n",
    "    rating_container = job.find('span', {'data-testid': 'holistic-rating'})\n",
    "    if rating_container:\n",
    "        return 'y', rating_container.text\n",
    "    else:\n",
    "        return 'n', None\n",
    "\n",
    "def get_job_salary(job):\n",
    "    '''Parse out job salary from html string. Many jobs have no salary, so a y/n field is also passed back to differentiate'''\n",
    "    salary_container = job.find('div', class_='metadata salary-snippet-container')\n",
    "    if salary_container:\n",
    "        salary_element = salary_container.find('div', {'data-testid': 'attribute_snippet_testid'})\n",
    "        if salary_element:\n",
    "            return 'y', salary_element.text\n",
    "        else: return 'n', None\n",
    "    else:\n",
    "        return 'n', None\n",
    "\n",
    "def get_job_url(job_uid):\n",
    "    '''Parse out job webpage url from html string'''\n",
    "    return f'https://ca.indeed.com/viewjob?jk={job_uid}'\n",
    "\n",
    "def get_current_str_date():\n",
    "    return datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "def get_job_attrs(job, job_id_dict):\n",
    "    '''\n",
    "    Accept html section of code relating to a single job and a reference dictionary of job id's from the Jobs database. \n",
    "    Ensures job found has a unique job id and, if so, parses out necessary elements of the job and populates into a pandas dataframe row. This is the main return value.\n",
    "    If element does not have a job id or it's a duplicate of an existing one, a Null value is returned which will skip that element.\n",
    "    '''\n",
    "    if job_is_ad(job):\n",
    "        return None\n",
    "\n",
    "    job_uid = get_job_uid(job, job_id_dict)\n",
    "    if job_uid is None:\n",
    "        return None\n",
    "        \n",
    "    job_title = get_job_title(job) \n",
    "    company = get_job_company(job)\n",
    "    location = get_job_location(job)\n",
    "    rating_provided, rating = get_company_rating(job)\n",
    "    salary_provided, salary = get_job_salary(job)\n",
    "    job_url = get_job_url(job_uid)\n",
    "    date_recorded = get_current_str_date()\n",
    "\n",
    "    new_row = {'id': job_uid, 'job_title': job_title, 'company': company, 'location': location, 'rating_provided': rating_provided, 'rating': rating,\n",
    "    'salary_provided': salary_provided, 'salary_text': salary, 'weblink': job_url, 'date_recorded': date_recorded}\n",
    "    row_to_add = pd.DataFrame(new_row, index = [0]).set_index('id')\n",
    "\n",
    "    return row_to_add\n",
    "\n",
    "def get_keyword_attrs(job, keyword_id_dict, search_term):\n",
    "    '''\n",
    "    Accept html section of code relating to a single job, a reference dictionary of job id's and keyword in the keyword database, and the search term used for the current job found.\n",
    "    Ensures job and keyword combination found is unique and, if so, populates the job id and keyword into a pandas dataframe row. This is the return value. \n",
    "    If element does not have a job id or it's a duplicate of an existing one, a Null value is returned which will skip that element. \n",
    "    ''' \n",
    "    if job_is_ad(job):\n",
    "        return None\n",
    "\n",
    "    job_uid = get_job_uid(job, keyword_id_dict)\n",
    "    if job_uid is None:\n",
    "        return None\n",
    "\n",
    "    new_row = {'id': job_uid, 'keyword': search_term}\n",
    "    row_to_add = pd.DataFrame(new_row, index = [0]).set_index('id')\n",
    "\n",
    "    return row_to_add\n",
    "\n",
    "def add_job_attributes_to_df(job_list, job_df, job_id_dict, keyword_df, keyword_id_dict, search_term):\n",
    "    '''\n",
    "    Loops through each listing in list provided. For each listing, calls functions to ensure listing is a job and if so, extract the relevant features provided for the job. \n",
    "    Returns this information as a pandas dataframe. Returns information for both job and keyword dataframes. \n",
    "    '''    \n",
    "    for job in job_list:\n",
    "        new_job = get_job_attrs(job, job_id_dict)\n",
    "        new_keyword = get_keyword_attrs(job, keyword_id_dict, search_term)\n",
    "\n",
    "        try:\n",
    "            if new_job is not None:\n",
    "                job_df = job_df.append(new_job, verify_integrity = True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if new_keyword is not None:    \n",
    "                keyword_df = keyword_df.append(new_keyword, verify_integrity = True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return job_df, keyword_df\n",
    "\n",
    "def append_job_info(search_term, search_location, page_limit = 100):\n",
    "    '''\n",
    "    Web scraper function to navigate to indeed, search for requested job title keywork (search_term), and loop through pages up to specified limit (page_limit).\n",
    "    The job data is extracted from the webpages and sent to necessary functions to parse. New jobs found are appended to the relevant pandas dataframes. \n",
    "    '''\n",
    "    job_df = create_initial_empty_df('job') \n",
    "    job_id_dict = get_searchable_dict('job') \n",
    "\n",
    "    keyword_df = create_initial_empty_df('keyword')\n",
    "    keyword_id_dict = get_searchable_dict('keyword', search_term)\n",
    "\n",
    "    driver = create_firefox_driver()\n",
    "    driver.get(f'https://ca.indeed.com/jobs?q={search_term}&l={search_location}&lang=en')\n",
    "    \n",
    "    for page in range(1,page_limit+1):    \n",
    "        if page > 1:\n",
    "            if not click_to_next_page(driver):\n",
    "                if not close_email_popup(driver):\n",
    "                    if not bypass_cloudflare_check(driver):\n",
    "                        print(f'reached last page of jobs at page: {page}, page requested: {page_limit}')\n",
    "                        driver.close()\n",
    "                        return job_df, keyword_df\n",
    "\n",
    "        job_list = parse_webpage_html(driver)\n",
    "        \n",
    "        job_df, keyword_df = add_job_attributes_to_df(job_list, job_df, job_id_dict, keyword_df, keyword_id_dict, search_term)\n",
    "\n",
    "    print(f'page reached: {page}, page requested: {page_limit}')\n",
    "    driver.close()    \n",
    "    return job_df, keyword_df\n",
    "\n",
    "\n",
    "def assign_salary_period(df):\n",
    "    '''\n",
    "    Determine the salary period used for the jobs and populate within dataframe. \n",
    "    Periods are typically yearly or hourly salaries; but monthly, weekly and daily periods also exist.\n",
    "    '''\n",
    "    df['salary_period'] = np.where(df['salary_text'].str.contains('year'), 'yearly',\n",
    "    np.where(df['salary_text'].str.contains('hour'), 'hourly',\n",
    "    np.where(df['salary_text'].str.contains('month'), 'monthly',\n",
    "    np.where(df['salary_text'].str.contains('day'), 'daily',\n",
    "    np.where(df['salary_text'].str.contains('week'), 'weekly',\n",
    "    np.NaN\n",
    "    )))))\n",
    "\n",
    "    return df\n",
    "\n",
    "def assign_salary_type(df):\n",
    "    '''\n",
    "    Determine the type of salary for the jobs and populte within dataframe. In this case, type is defined as either range, floor, ceiling or expected. \n",
    "    Most of the time, the salary is a range between 2 values (range) or a single value (expected). \n",
    "    Sometimes, the top (ceiling) or bottom (floor) of the salary is provided instead (ie. \"From $x\" or \"To $x\" respectively).\n",
    "    '''\n",
    "    df['salary_type'] = np.where(df['salary_text'].str.contains('–'), 'range',\n",
    "    np.where(df['salary_text'].str.contains('From'), 'floor',\n",
    "    np.where(df['salary_text'].str.contains('Up'), 'ceiling',\n",
    "    np.where(df['salary_text'].str.startswith('$'), 'expected',\n",
    "    np.NaN\n",
    "    ))))\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_salary_str_to_float(salary_str, salary_type = None, column = None):\n",
    "    '''Convert a salary string to a floating point number.'''\n",
    "    if salary_type == 'range':\n",
    "        if column == 'min':\n",
    "            return float(salary_str.split('–')[0].replace('$','').replace(',',''))\n",
    "        if column == 'max':\n",
    "            return float(salary_str.split('–')[1].split()[0].replace('$','').replace(',',''))\n",
    "\n",
    "    return float(salary_str.split('$')[1].split()[0].replace(',',''))\n",
    "\n",
    "def get_actual_salary_values(row):\n",
    "    '''\n",
    "    Defines and populates 3 salary columns. \"Floor\" represents the minimum salary, \"Ceiling\" represents the maximum salary, and \"Expected\" represents the expected salary. \n",
    "    The \"Range\" salary type has a floor and ceiling value with the a calculated expected being linearlly interpolated between the two. The rest only have one salary value corresponding to their respective type. \n",
    "    '''\n",
    "    if row['salary_type'] == 'range':\n",
    "        row['floor'] = convert_salary_str_to_float(row['salary_text'], 'range', 'min')\n",
    "        row['ceiling'] = convert_salary_str_to_float(row['salary_text'], 'range', 'max')\n",
    "        row['expected'] = round((row['ceiling'] + row['floor'])/2.0,2)\n",
    "        return row\n",
    "    if row['salary_type'] == 'expected':\n",
    "        row['expected'] = convert_salary_str_to_float(row['salary_text'])\n",
    "        return row\n",
    "    if row['salary_type'] == 'ceiling':\n",
    "        row['ceiling'] = convert_salary_str_to_float(row['salary_text'])\n",
    "        return row\n",
    "    if row['salary_type'] == 'floor':\n",
    "        row['floor'] = convert_salary_str_to_float(row['salary_text'])\n",
    "        return row\n",
    "        \n",
    "def create_salary_df(job_df):\n",
    "    '''Use jobs with salaries provided to create a dataframe with salary numbers for those jobs.'''\n",
    "    salary_df = job_df[job_df['salary_provided'] == 'y'][['salary_text']]\n",
    "\n",
    "    salary_df = (\n",
    "        salary_df.pipe(assign_salary_period).pipe(assign_salary_type).apply(get_actual_salary_values, axis=1)\n",
    "    )\n",
    "    return salary_df\n",
    "\n",
    "\n",
    "def assign_job_location_type(job_df):\n",
    "    '''\n",
    "    Defines a location \"type\" based on presence or absence of \"in\" or \",\" in the location string. The location fields that can be parsed from the string will depend on the location type determined.\n",
    "    The typical format for a location string is: \"[Location Model] in [city], [jurisdiction]. Where location model is \"Remote\", \"Hybrid Remote\", etc. \n",
    "    However, all three fields there are optional so the parsing has to be able to account for this. \n",
    "    '''\n",
    "    job_df['location_type'] = np.where(~job_df['location'].str.contains(' in |,', regex = True), 'neither',\n",
    "    np.where(~job_df['location'].str.contains(' in ') & job_df['location'].str.contains(','), 'comma',\n",
    "    np.where(job_df['location'].str.contains(' in ') & ~job_df['location'].str.contains(','), 'in',\n",
    "    np.where(job_df['location'].str.contains(' in ') & job_df['location'].str.contains(','), 'both',\n",
    "    np.NaN))))\n",
    "\n",
    "    return job_df\n",
    "\n",
    "def get_job_location_model(row):\n",
    "    '''Parse out location model based on location string and location type. This is done on a row basis.'''\n",
    "    if row['location_type'] == 'neither':\n",
    "        if row['location'] in ['Remote', 'Remote Hybrid']:\n",
    "            row['location_model'] = row['location']\n",
    "            return row\n",
    "\n",
    "    elif row['location_type'] == 'comma':\n",
    "        row['location_model'] = 'Not Specified'\n",
    "        return row\n",
    "\n",
    "    elif row['location_type'] == 'in' or row['location_type'] == 'both':\n",
    "        row['location_model'] = row['location'].split(' in ')[0]\n",
    "        return row\n",
    "    \n",
    "    row['location_model'] = 'Not Specified'\n",
    "    return row\n",
    "\n",
    "def check_if_jurisdiction(location_str):\n",
    "    '''Checks to see if location string contains a reference to a Canadian jurisdiction (province/territory)'''\n",
    "    \n",
    "    canada_jurisdictions = {'Alberta' : 'AB', 'British Columbia': 'BC', 'Manitoba': 'MB', 'New Brunswick': 'NB', 'Newfoundland and Labrador': 'NL', 'Nova Scotia': 'NS', 'Ontario': 'ON', \n",
    "    'Prince Edward Island': 'PE', 'Quebec': 'QC', 'Saskatchewan': 'SK', 'Northwest Territories': 'NT', 'Nunavut': 'NU', 'Yukon': 'YT'}\n",
    "\n",
    "    if location_str in canada_jurisdictions:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_if_country(location_str):\n",
    "    '''At times, the location string provides just the country and no other information - this is a check for that.'''\n",
    "    if location_str in ['Canada']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_abbreviated_jurisdiction(jurisdiction_str):\n",
    "    '''Returns an abbreviated form of the canadian jurisdiction based on the full name string provided.'''\n",
    "\n",
    "    canada_jurisdictions = {'Alberta' : 'AB', 'British Columbia': 'BC', 'Manitoba': 'MB', 'New Brunswick': 'NB', 'Newfoundland and Labrador': 'NL', 'Nova Scotia': 'NS', 'Ontario': 'ON', \n",
    "    'Prince Edward Island': 'PE', 'Quebec': 'QC', 'Saskatchewan': 'SK', 'Northwest Territories': 'NT', 'Nunavut': 'NU', 'Yukon': 'YT'}\n",
    "\n",
    "    return canada_jurisdictions.get(jurisdiction_str, 'Not Found')\n",
    "\n",
    "def get_job_city(row):\n",
    "    '''Parse out city based on location string and location type. This is done on a row basis.'''\n",
    "    if row['location_type'] == 'neither':\n",
    "        if not check_if_jurisdiction(row['location']) and not check_if_country(row['location']):\n",
    "            row['city'] = row['location']\n",
    "            return row\n",
    "\n",
    "    elif row['location_type'] == 'comma':\n",
    "        row['city'] = row['location'].split(',')[0]\n",
    "        return row\n",
    "\n",
    "    elif row['location_type'] == 'in':\n",
    "        row['city'] = 'Not Specified'\n",
    "    \n",
    "    elif row['location_type'] == 'both':\n",
    "        row['city'] = row['location'].split(',')[0].split(' in ')[1]\n",
    "        return row\n",
    "    \n",
    "    row['city'] = 'Not Specified'\n",
    "    return row\n",
    "\n",
    "def get_job_jurisdiction(row):\n",
    "    '''Parse out juristiction based on location string and location type. This is done on a row basis.'''\n",
    "    if row['location_type'] == 'neither':\n",
    "        if check_if_jurisdiction(row['location']):\n",
    "            row['jurisdiction'] = get_abbreviated_jurisdiction(row['location'])\n",
    "            return row\n",
    "\n",
    "    elif row['location_type'] == 'comma':\n",
    "        row['jurisdiction'] = row['location'].split(', ')[1]\n",
    "        return row\n",
    "\n",
    "    elif row['location_type'] == 'in':\n",
    "        location_to_check = row['location'].split(' in ')[1]\n",
    "        if check_if_jurisdiction(location_to_check):\n",
    "            row['jurisdiction'] = get_abbreviated_jurisdiction(location_to_check)\n",
    "            return row\n",
    "    \n",
    "    elif row['location_type'] == 'both':\n",
    "        row['jurisdiction'] = row['location'].split(', ')[1]\n",
    "        return row\n",
    "    \n",
    "    row['jurisdiction'] = 'Not Specified'\n",
    "    return row\n",
    "\n",
    "def get_job_country(df, country):\n",
    "    '''Parse out country based on location string.'''\n",
    "    df['country'] = country\n",
    "    return df\n",
    "\n",
    "def get_job_location_attrs(job_df, country):\n",
    "    '''Pass dataframe into various functions that parse out location attributes from the location string using method chaining. Parsed attributes are populated into the database and returned.'''\n",
    "    job_df_with_location_data = (\n",
    "        job_df.pipe(assign_job_location_type).apply(get_job_location_model, axis=1).apply(get_job_city, axis=1).apply(get_job_jurisdiction, axis=1).pipe(get_job_country, country)\n",
    "    )\n",
    "\n",
    "    return job_df_with_location_data[['job_title','company','location','rating_provided','rating','salary_provided','weblink','date_recorded','location_model','country', 'jurisdiction','city']]\n",
    "\n",
    "\n",
    "def append_pandas_to_sql(df, df_type):\n",
    "    '''Take dataframes created and populated by program with new jobs found and append them to the existing SQL tables.'''\n",
    "\n",
    "    engine = sqlalchemy.create_engine('sqlite:///JobData.db')\n",
    "\n",
    "    if df_type == 'job':\n",
    "        try:\n",
    "            df.to_sql('Jobs', engine, if_exists='append')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'Cannot add to job table. Error raised: {e}')\n",
    "    if df_type == 'keyword':\n",
    "        try:\n",
    "            df.to_sql('KeywordRef', engine, if_exists='append')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'Cannot add to keyword table. Error raised: {e}')\n",
    "    if df_type == 'salary':\n",
    "        try:\n",
    "            df.to_sql('Salaries', engine, if_exists = 'append')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'Cannot add to salary table. Error raised: {e}')\n",
    "\n",
    "def get_indeed_job_data(search_terms, country = 'Canada', page_limit = 15):\n",
    "    for keyword in search_terms:\n",
    "        job_df, keyword_df = append_job_info(keyword, country, page_limit)\n",
    "\n",
    "        salary_df = create_salary_df(job_df)\n",
    "\n",
    "        job_df_with_location_data = get_job_location_attrs(job_df, country)\n",
    "\n",
    "        print(f'New {keyword} jobs found to be added: {len(job_df_with_location_data.index)}')\n",
    "\n",
    "        append_pandas_to_sql(job_df_with_location_data, 'job')\n",
    "        append_pandas_to_sql(keyword_df, 'keyword')\n",
    "        append_pandas_to_sql(salary_df, 'salary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_indeed_job_data(['data analyst', 'data scientist', 'business intelligence', 'database administrator'], 'Canada', 5)\n",
    "get_indeed_job_data(['database administrator'], 'Canada', 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
